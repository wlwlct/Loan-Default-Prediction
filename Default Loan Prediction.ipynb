{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Predict whether the accepted loan would be properly paid back or not\n",
    "\n",
    "log:\n",
    "drop columns and load all data\n",
    "move on to feature engineering\n",
    "\n",
    "\n",
    "Make it better:\n",
    "Can we use forward and backward or stepwise method to select the important parameter?\n",
    "\n",
    "Should we treat ordinal value as catagory values or numeric values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "reader = pd.read_csv('accepted_2007_to_2018q4.csv/accepted_2007_to_2018Q4.csv', chunksize=10000)\n",
    "mid=[]\n",
    "for chunk in reader:\n",
    "    chunk=chunk[chunk['loan_status'].apply( lambda x: x not in ['Does not meet the credit policy. Status:Fully Paid', 'Does not meet the credit policy. Status:Charged Off'])]\n",
    "    # drop columns with more than 50% missing values\n",
    "    chunk.rename(columns={'verification_status_joint':'verified_status_joint'}, inplace=True)\n",
    "\n",
    "    chunk = chunk.drop(columns= ['verified_status_joint','sec_app_mths_since_last_major_derog', 'sec_app_revol_util',\n",
    "       'revol_bal_joint', 'sec_app_inq_last_6mths',\n",
    "       'sec_app_collections_12_mths_ex_med',\n",
    "       'sec_app_chargeoff_within_12_mths', 'sec_app_num_rev_accts',\n",
    "       'sec_app_open_acc', 'sec_app_mort_acc', 'sec_app_fico_range_high',\n",
    "       'sec_app_fico_range_low', 'dti_joint', 'annual_inc_joint',\n",
    "       'mths_since_last_record', 'mths_since_recent_bc_dlq',\n",
    "       'mths_since_last_major_derog', 'mths_since_recent_revol_delinq',\n",
    "       'mths_since_last_delinq'])\n",
    "\n",
    "    #  drop columns with more than 20% of missing values\n",
    "    chunk = chunk.drop(columns=['open_acc_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il',\n",
    "       'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc',\n",
    "       'all_util', 'inq_fi', 'total_cu_tl', 'inq_last_12m'])\n",
    "\n",
    "    # drop column without explaination\n",
    "    chunk = chunk.drop(columns=['debt_settlement_flag', 'debt_settlement_flag_date', 'deferral_term',\\\n",
    "         'disbursement_method', 'hardship_amount', 'hardship_dpd', 'hardship_end_date', 'hardship_flag',\\\n",
    "            'hardship_last_payment_amount', 'hardship_length', 'hardship_loan_status', 'hardship_payoff_balance_amount', \\\n",
    "            'hardship_reason', 'hardship_start_date', 'hardship_status', 'hardship_type', 'open_act_il', \\\n",
    "            'orig_projected_additional_accrued_interest', 'payment_plan_start_date', 'sec_app_open_act_il',\\\n",
    "            'settlement_amount', 'settlement_date', 'settlement_percentage', 'settlement_status', 'settlement_term'])\n",
    "    # drop column with high correlation\n",
    "    chunk = chunk.drop(columns=['out_prncp_inv','funded_amnt','funded_amnt_inv','tot_hi_cred_lim','total_il_high_credit_limit'])\n",
    "    # columns that are not related to prediction\n",
    "    chunk = chunk.drop(columns=['url','desc','member_id','id','emp_title','sub_grade','zip_code','policy_code'])\n",
    "\n",
    "    # drop column with date\n",
    "    chunk = chunk.drop(columns= ['issue_d','earliest_cr_line','last_pymnt_d','next_pymnt_d','last_credit_pull_d','sec_app_earliest_cr_line'])\n",
    "\n",
    "    mid.append(chunk)\n",
    "\n",
    "accepted = pd.concat(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the imported data\n",
    "accepted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description from dataset:\n",
    "data_descriptions = pd.read_excel('LCDataDictionary.xlsx', sheet_name=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the following, we will try to analysis the column information and remvovee non-related columns. To do this, our first step is to understand the meaning of each column. The data set is acompanied with a well-documented data description. We will use it as a guideline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns that are not well defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_descriptions[1] = data_descriptions[1].iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the descriptions to one sheet\n",
    "data_descriptions[0] = data_descriptions[0][['LoanStatNew', 'Description']]\n",
    "data_descriptions[1].columns =['LoanStatNew', 'Description']\n",
    "data_descriptions = pd.concat([data_descriptions[0], data_descriptions[1]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearly there is an extra space need to be dealt with.\n",
    "data_descriptions.LoanStatNew = data_descriptions.LoanStatNew.str.strip()\n",
    "# verified status joint is called verification status joint in the dataset\n",
    "accepted.rename(columns={'verification_status_joint':'verified_status_joint'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na\n",
    "data_descriptions = data_descriptions[[pd.notna(i) for i in data_descriptions.LoanStatNew]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which column does not have an description\n",
    "print('column name in data without description')\n",
    "print(sorted(set(accepted.columns)-set(data_descriptions.LoanStatNew))) # col in data without description\n",
    "print('*'*100)\n",
    "print(sorted(set(data_descriptions.LoanStatNew)-set(accepted.columns)  )) # columns with description not in data\n",
    "# The columns which does not contain a proper description will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that do not have a proper explaination\n",
    "accepted.drop(columns=list(set(accepted.columns)-set(data_descriptions.LoanStatNew)), inplace=True)\n",
    "accepted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general examine data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earliest_cr_line, last_pymnt_d, next_pymnt_d, last_credit_pull_d suppose to be datetime month_label-Year\n",
    "#for col in ['earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d','issue_d','sec_app_earliest_cr_line']:\n",
    "#    accepted[col] = pd.to_datetime(accepted[col], format='%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert id into object\n",
    "#accepted[['id','policy_code']]=accepted[['id','policy_code']].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check existing dtypes\n",
    "accepted.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int\n",
    "accepted.select_dtypes('int64').columns # id should be cat value\n",
    "# <M8[ns]\n",
    "datetime_dict={col: [ accepted[col].unique()] for col in accepted.select_dtypes('<M8[ns]').columns}\n",
    "# float\n",
    "float_dict={col: [ accepted[col].unique()] for col in accepted.select_dtypes('float64').columns}\n",
    "# 0\n",
    "object_dict={col: [accepted[col].unique()] for col in accepted.select_dtypes('O').columns}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unique datetime\n",
    "would not necessary for prediction\n",
    "\n",
    "potential: time interval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique datetime, generate date_col\n",
    "pd.DataFrame.from_dict(datetime_dict, orient='index').rename(columns={0:'unique_values'})\n",
    "date_col = ['issue_d','earliest_cr_line','last_pymnt_d','next_pymnt_d','last_credit_pull_d','sec_app_earliest_cr_line']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unique object values\n",
    "- identify columns that are not necessary: \n",
    "id, url, zip_code, grade/subgrade, title?, emp_title?\n",
    "\n",
    "\n",
    "- test whether data type is proper or not\n",
    "- define values for prediction\n",
    "- clean nan values\n",
    "- chisq independent test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique object values\n",
    "object_dict_unique = pd.DataFrame.from_dict(object_dict, orient='index').rename(columns={0:'unique_values'})\n",
    "object_dict_unique['n']=object_dict_unique['unique_values'].apply(len)\n",
    "object_dict_unique.sort_values(\"n\")\n",
    "# not related columns: id, url, desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean title (deal with unstructured data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value that contains loan\n",
    "accepted['title'].str.lower().str.replace(' loan','').str.contains('loan').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of stopwords and remove stopwords\n",
    "stop_words1=['just','a','the','my','need','help','to', 'one', 'me', 'for','smart', 'jc', 'low', 'looking', 'lower', 'mike\\'s', 'mike',\\\n",
    "    'michelle\\'s', 'michelle', 'many', 'needed', 'mission', 'dad\\'s', 'seeking', 'high', 'it', 'new', 'nice', 'and','quick','next','level', 'more',\\\n",
    "        'large','small','lendingclub','better', 'me', 'you','beautiful','easy', 'finally', 'rescue', 'get','first','last','second', 'up','lower',\\\n",
    "            'combine','little', 'project','please', 'thank', 'thanks','ny','of','is','are','i','on','&','this','in','me,','be','with','from','-',\\\n",
    "                'big','short','end','our', 'needs', 'bye','two','over','will','at','some','do','clear','combine','no','or']\n",
    "accepted['title'] = accepted['title'].apply( lambda x: ' '.join([s for s in x.split() if (s not in stop_words1)]) if pd.notna(x) else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### value recode\n",
    "stop_words2=['lending club','no more','-','\\s','\\d','$','best','better','big','bye','final','buy','finish','going','good','great','hard','high',\\\n",
    "    'happy','honest','less','than','responsible','rich','right','short','long','smart','smile','term','want','unexpected','the','project','new','mine',\n",
    "    'profit','first','second','expense']\n",
    "accepted['title'] = accepted['title'].str.lower().str.replace(' loan','').str.replace('loan ','').str.strip('!,\\'-.?&#$\\/\\ \\\\0123456789%:+=\\\"_)(')\n",
    "accepted['title'] = accepted['title'].str.replace('c.c.','credit card')\n",
    "accepted['title'] = accepted['title'].str.replace('lc','lending club')\n",
    "accepted['title'] = accepted['title'].str.replace('cc','credit card')\n",
    "\n",
    "for word in stop_words2:\n",
    "    accepted['title'] = accepted['title'].str.replace(word,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine rows that contain certain word\n",
    "#### grade and subgrade, if grade is empty, subgrade is empty\n",
    "#accepted[['grade', 'sub_grade']].isna().sum(axis=1).unique()\n",
    "#### moving is basically physically moving\n",
    "accepted['title'][accepted['title'].apply(lambda x: 'moving' in x if pd.notna(x) else False)]\n",
    "#### mustang is car\n",
    "accepted['title'][accepted['title'].apply(lambda x: 'mustang' in x if pd.notna(x) else False)]\n",
    "#### banks means people dont like the idea of bank\n",
    "accepted['title'][accepted['title'].apply(lambda x: 'banks' in x if pd.notna(x) else False)]\n",
    "#### mediacal is medial expanse\n",
    "accepted['title'][accepted['title'].apply(lambda x: 'med' in x if pd.notna(x) else False)]\n",
    "#### mba is school\n",
    "accepted['title'][accepted['title'].apply(lambda x: 'mba' in x if pd.notna(x) else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted['title'][accepted['title'].apply(lambda x: 'wash' in x if pd.notna(x) else False)].value_counts().head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 10 categories for recode based on the rule\n",
    "reword={'debt':['deb','debt','dedt','dbt','dept','bill','debit','wells','chase','citi','visa','bankamerica','bankofamerica','barclay','amex','american','boa','bofa','credit card',\\\n",
    "        'creditcard','credit','card','pay','pay-off','payoff','pay off','off','payback','paid','discover'],\\\n",
    "    'consolidation':['consolidation','con','cos','capitolslate','consalidadtion','conso','reconciliation','consoildation','consolidation','consolodation',\\\n",
    "        'consoldate','consolitation','consolidate','consol','cosolidation','onsolidation','recon','consol','refinance','refi','re-fi'],\\\n",
    "    'medical':['med','hospital','dental','health','surgery','dentist','rehab','headache','doctor'],\\\n",
    "    'wedding':['diamondring','wed','engage','honey','wedding','engagement','marr','ring'],\\\n",
    "    'mbuy':['mus','major','equip','daniel','defense','appliance','computer','laptop','camera','purchase','perchase','porchase',\\\n",
    "        'golf','boat','purchase','gun','software','seadoo','ship'],\\\n",
    "    'vehicle':['motor','dodge','harley','kawa','chevy','bmw','ford','toyota','wheels','honda','scooter','vehic','truck','mustang','subaru','suzuki',\n",
    "        'mazda','auto','mercedes','auto','fuel','car','transmission','jeep','bike','trailer','subaru','nissan','engine','volvo','truck'],\\\n",
    "    'emergency':['emer','emr','emergency'],\\\n",
    "    'moving':['crossc','relo','moving','move'],\\\n",
    "    'law':['legal','attorney','law'],\\\n",
    "    'school':['edu','exam','training','classes','school','mba','mster\\'s','student','graduate','phd','education','course','tuition','book','college',\n",
    "        'teacher','program'], \n",
    "    'business':['farm','invent','business','buiness','buis','bus','bakery','shop','studio','web','buisness','company','busines','start-up',\n",
    "        'startup','start up','inves'],\\\n",
    "    'home':['hous','heat','condo','chimney','apart','build','barn','basement','bassment','bath','boil','borrow',\\\n",
    "        'driveway','sewer','solar','property','cabin','yard','office','lawn','basement','renovat','home','mortgage','tub','pool','roof','rent',\\\n",
    "        'garage','bathroom','bedroom','kitchen','outdoor','suite','room','floor','ceil','garden','house','window','deck','fence','remodel','a/c','furniture','bed',\n",
    "        'furnace','landscape','shelter','remo','tree','wash','lighttunnel'],\\\n",
    "    'personal':['presonal','pesonel','priv','peronal','personnel','personal','person','personal','vacation','money','cash','trip','pers','pes'],\\\n",
    "    'family':['child','adop','brother','sister','baby','mom','father','mother','grand','dad','daughter','kid','fam','funeral','myson'],\\\n",
    "    'other':['making','catch','eas','chan','fix','hop','impr','clear','com','never','add','bad','break','bright','help',\\\n",
    "        'bridge','sum','bless','blue','back','balan','insurance','together','all','goal','god','no','temp','self','free',\\\n",
    "        'live','lend','start','breath','day','clean','dream','love','peace','jan','feb','march','april','may','june','july',\\\n",
    "        'august','september','october','november','december','financ','redu','stres','soul','luck','month','opera','reduce',\\\n",
    "        'spring','time','out','plan','clos','capi','life','begin','mistake','sav','relief','air','tax','apr','interest','banks','green','vaction','simp']\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode all the values to 10 category.\n",
    "for k,v in reword.items():\n",
    "    for item in v:\n",
    "        accepted['title']=accepted['title'].apply(lambda x: (k if item in x else x) if isinstance(x,str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace certain values to other\n",
    "accepted['title'] = accepted['title'].replace({'or':'other','loan':'other','':'other','myloan':'other','future':'other','s':'other','k':'other',\n",
    "'my':'other'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique value of each category\n",
    "test = accepted[['title']].value_counts()\n",
    "plt.plot(range(len(test)), test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename value to other if they are not in the top 10 categories\n",
    "top10_index = [col[0] for col in accepted[['title']].value_counts().iloc[:10].index]\n",
    "accepted['title'] = accepted['title'].apply(lambda x: (x if x in top10_index else 'other') if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check the cleaned categories.\n",
    "accepted[['title']].value_counts().head(20)\n",
    "#[test.append(i[0].split()) if isinstance(i[0].split(),str) else test.extend(i[0].split()) for i in accepted[['title']].value_counts().keys()]\n",
    "#from collections import Counter\n",
    "#sorted(Counter(test).items(), key=lambda x: x[1], reverse=True)\n",
    "#sorted(Counter(test).items(), key=lambda x: x[0])[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### emp title 436961 unique\n",
    "#len(accepted['emp_title'].str.lower().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine unique value in each object col and generate col name not reelated to the prediction.\n",
    "not_related_cols = ['id','emp_title','sub_grade','zip_code','policy_code']\n",
    "object_dict_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define loan status value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column Explaination**\n",
    "\n",
    "revolving loan: A revolving loan facility is a form of credit issued by a financial institution that provides the borrower with the ability to draw down or withdraw, repay, and withdraw again. A revolving loan is considered a flexible financing tool due to its repayment and re-borrowing accommodations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loan status explaination**\n",
    "\n",
    "Current: Loan is up to date on all outstanding payments. / any loan that is fully paid to date according to a contract.\n",
    "\n",
    "Fully paid: Loan has been fully repaid, either at the expiration of the 3- or 5-year year term or as a result of a prepayment.\n",
    " \n",
    " \n",
    "In Grace Period: Loan is past due but within the 15-day grace period. \n",
    " \n",
    "Late (16-30): Loan has not been current for 16 to 30 days. Learn more about the tools LendingClub has to deal with delinquent borrowers.\n",
    " \n",
    "Late (31-120): Loan has not been current for 31 to 120 days. Learn more about the tools LendingClub has to deal with delinquent borrowers.\n",
    " \n",
    "Default: Loan has not been current for an extended period of time. \n",
    "\n",
    "Charged Off: Loan for which there is no longer a reasonable expectation of further payments. Upon Charge Off, the remaining principal balance of the Note is deducted from the account balance.\n",
    "\n",
    "Default and charged-off:\n",
    "About 4-6 months after you miss your first payment, your loan will default and then charge-off. \n",
    "\n",
    "**From the defination above, charged off and default would be treated as default**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check unique value counts in the column we want to predict\n",
    "accepted['loan_status'].value_counts()\n",
    "# From the table bellow, does not meet the credit policy would be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remvove rows that loan_status contains nan\n",
    "accepted.dropna(subset=['loan_status'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if charged off or default then treat as default, rest are not default\n",
    "accepted['Default']=accepted['loan_status'].apply(lambda x: 1 if x in ['Default', 'Charged Off'] else 0).astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the percentage of missing values\n",
    "accepted[object_dict_unique.index].isnull().mean()*100\n",
    "# drop verified_status_joint has more than 94 percent of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing cat value with mode\n",
    "#accepted.drop(columns='loan_status', inplace=True)\n",
    "accepted['emp_length']=accepted['emp_length'].fillna(accepted['emp_length'].mode()[0])\n",
    "accepted['title']=accepted['title'].fillna(accepted['title'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unique numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique numeric values\n",
    "float_dict_unique=pd.DataFrame.from_dict(float_dict, orient='index').rename(columns={0:'unique_values'})\n",
    "float_dict_unique['n']=float_dict_unique['unique_values'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find column with single value\n",
    "float_dict_unique[float_dict_unique['n']==1].index\n",
    "# single value: 'member_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find column with less than 10% of unique values. (1000 for 10000)\n",
    "unique_index_1000=float_dict_unique[float_dict_unique['n']>1000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "single_value_index=[]\n",
    "\n",
    "numeric_cols=float_dict_unique.drop(index=single_value_index)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(numeric_cols.sort_values('n'))\n",
    "\n",
    "# policy_code is cat value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of 0s\n",
    "float_zeros = pd.DataFrame((accepted[float_dict_unique.index]==0).mean())\n",
    "float_zeros.columns=['percentage of 0s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all missing values\n",
    "accepted.isnull().any(axis=1).sum()/2257952 # 0.9998609359277788 not possible to remove all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove columns with missing  values more than 50%\n",
    "float_dict_missing = accepted[float_dict_unique.index].isnull().mean()*100\n",
    "float_dict_missing = float_dict_unique.join(float_dict_missing.rename('missing percentage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of misssing\n",
    "float_dict_missing['missing percentage'].sort_values().plot(kind='bar')\n",
    "plt.ylabel('percentage of missing values')\n",
    "plt.xlabel('column names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with missing value more than 20%s\n",
    "float_dict_missing[float_dict_missing['missing percentage']>20].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check the number of rows with full set of data\n",
    "accepted[float_dict_missing.index].isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    def df_unique_missing_0s(df):\n",
    "    numeric_cols=float_dict_unique.drop(index=single_value_index)\n",
    "    # percentage of 0s\n",
    "    float_zeros = pd.DataFrame((df[float_dict_unique.index]==0).mean())\n",
    "    float_zeros.columns=['percentage of 0s']\n",
    "    # missings\n",
    "    float_dict_missing = df[float_dict_unique.index].isnull().mean()*100\n",
    "    float_dict_missing = float_dict_unique.join(float_dict_missing.rename('missing percentage'))\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(numeric_cols.join(float_zeros).join(float_dict_missing['missing percentage']).\\\n",
    "        sort_values(['missing percentage','percentage of 0s'], ascending=False))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accepted_trimed=accepted.drop(columns='mths_since_recent_inq').dropna()\n",
    "# could not use trimed version because it significantly remove default data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of unique value, percentage of 0s in the list, and persentage of missing values.\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(numeric_cols.join(float_zeros).join(float_dict_missing['missing percentage']).\\\n",
    "        sort_values(['missing percentage','percentage of 0s'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the correlation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted['Default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the correlation of the column that contains numeric values\n",
    "accepted_numeric_corr = accepted[unique_index_1000].drop(columns=['recoveries','collection_recovery_fee']).corr()\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(accepted_numeric_corr,vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate colname and correlations\n",
    "accepted_numeric_corr_melt = accepted_numeric_corr.reset_index(drop=False).melt('index', var_name='var2', value_name='corr')\n",
    "accepted_numeric_corr_melt = accepted_numeric_corr_melt.rename(columns={'index':'var1'})\n",
    "highly_corr_cols = accepted_numeric_corr_melt[(abs(accepted_numeric_corr_melt['corr'])>0.5) & (accepted_numeric_corr_melt['var1']!=accepted_numeric_corr_melt['var2'])]\n",
    "highly_corr_cols['combine'] = (highly_corr_cols['var1']+highly_corr_cols['var2']).apply(lambda x: ''.join(sorted(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    highly_corr_cols=highly_corr_cols.sort_values('corr', ascending=False).drop_duplicates(subset='combine')\n",
    "    display(highly_corr_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation with corr>0.5\n",
    "for v1,v2 in highly_corr_cols[['var1','var2']].values:\n",
    "    print(v1,v2)\n",
    "    plt.figure()\n",
    "    #plt.scatter(accepted[v1], accepted[[v1,v2]].groupby(v1).mean().reset_index()[v2])\n",
    "    accepted[[v1,v2]].groupby(v1).mean().reset_index().plot(kind='scatter', x=v1,y=v2)\n",
    "    plt.show()\n",
    "# drop on in:\n",
    "# out_prncp/out_prncp_inv; funded_amnt/loan_amnt; loan_amnt/funded_amnt_inv; tot_cur_bal/tot_hi_cred_lim;\n",
    "# total_il_high_credit_limit/total_bal_ex_mort; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_prncp/out_prncp_inv; tot_cur_bal/tot_hi_cred_lim;\n",
    "# total_il_high_credit_limit/total_bal_ex_mort; \n",
    "high_corr_cols = ['out_prncp_inv','funded_amnt','funded_amnt_inv','tot_hi_cred_lim','total_il_high_credit_limit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**delinquent**: Delinquency means that you are behind on payments. Once you are delinquent for a certain period of time (usually nine months for federal loans), your lender will declare the loan to be in default. The entire loan balance will become due at that time.\n",
    "\n",
    "**trades**: Trade finance represents the financial instruments and products that are used by companies to facilitate international trade and commerce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_dict_missing[float_dict_missing['missing percentage']>0].sort_values('n', ascending=False)\n",
    "# below 20 use mode, above 20 use median\n",
    "for col in float_dict_missing.index:\n",
    "    if float_dict_missing.loc[col,'n']>20:\n",
    "        accepted[col].fillna(accepted[col].median(),inplace=True)\n",
    "    else:\n",
    "        accepted[col].fillna(accepted[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any missing value in the df\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(pd.DataFrame(accepted.isnull().mean()).sort_values(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cleaned dataset\n",
    "accepted_cleaned = pd.read_csv('./cleaned dataset.csv')\n",
    "accepted_cleaned.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "accepted_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(accepted_cleaned['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of the of each type\n",
    "for col in accepted_cleaned.select_dtypes('object').columns:\n",
    "    sns.countplot(accepted_cleaned[col])\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in accepted_cleaned.select_dtypes('O').columns:\n",
    "    try:\n",
    "        current_table = pd.pivot_table(accepted_cleaned, values='loan_amnt', index=col, columns='loan_status', aggfunc='count')\n",
    "        sns.heatmap(current_table)\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of object\n",
    "for col in accepted_cleaned.select_dtypes('float').columns:\n",
    "    sns.boxplot(y=col, x='loan_status',  data=accepted_cleaned)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_cleaned['term'] = accepted_cleaned['term'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and check\n",
    "X_train, X_test, y_train, y_test= train_test_split(accepted_cleaned.drop(columns='loan_status'), accepted_cleaned['loan_status'], test_size=0.75, random_state=42)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote for imbalanced data\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ColumnTransform = ColumnTransformer(transformers=[\n",
    "    ('numeric', StandardScaler(), X_train.select_dtypes(include='number').columns.tolist()),\n",
    "    ('categorical', OneHotEncoder(drop='first'), X_train.select_dtypes(include='category').columns.tolist())\n",
    "])\n",
    "X_train = ColumnTransform.fit_transform(X_train)\n",
    "X_test = ColumnTransform.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = RandomUnderSampler()\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "FI_RF = RandomForestClassifier()\n",
    "FI_RF.fit(X_train, y_train)\n",
    "FI_RF_importances = FI_RF.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "FI_LR  = LogisticRegression()\n",
    "FI_LR.fit(X_train, y_train)\n",
    "\n",
    "FI_LR_importances = FI_LR.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "FI_tree = DecisionTreeClassifier()\n",
    "FI_tree.fit(X_train, y_train)\n",
    "FI_tree_importance = FI_tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "FI_XGboost = XGBClassifier()\n",
    "FI_XGboost.fit(X_train, y_train)\n",
    "FI_XGboost_importance = FI_XGboost.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1); plt.plot(FI_RF.feature_importances_, label='Random Forest')\n",
    "plt.subplot(2,2,2); plt.plot(np.abs(FI_LR.coef_[0]), label = 'Logistic Regression')\n",
    "plt.subplot(2,2,3); plt.plot(FI_tree.feature_importances_, label = 'Decision Tree')\n",
    "plt.subplot(2,2,4); plt.plot(FI_XGboost.feature_importances_, label = 'XGboost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "selected_features= np.abs( \\\n",
    "    minmax.fit_transform(FI_RF_importances.reshape(-1,1)) * \\\n",
    "    minmax.fit_transform(FI_LR_importances.reshape(-1,1)) * \\\n",
    "    minmax.fit_transform(FI_tree_importance.reshape(-1,1)) * \\\n",
    "    minmax.fit_transform(FI_XGboost_importance.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(selected_features>0.001)\n",
    "selected_idx = np.squeeze(np.argsort(selected_features, axis=0))\n",
    "columns = accepted_cleaned.columns[selected_idx[-10:]]\n",
    "print(columns)\n",
    "plt.plot(np.sort(selected_features[-10:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create selected cols\n",
    "selected_cols = selected_idx[-10:] # selected_features[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting and Pipeline build\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resplit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,selected_cols]\n",
    "X_test = X_test[:,selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps = [('RF', RandomForestClassifier())]\n",
    "params = {'RF__n_estimators': [10], 'RF__criterion':['gini'], 'RF__max_depth':[4] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_C = Pipeline(steps=Steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV(RF_C, param_grid=params ,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_C.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF_C.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = RF_C.predict_proba(X_test) # The first index refers to the probability that the data belong to class 0, and the second refers to the probability that the data belong to class 1.\n",
    "false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_prob[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(false_positive_rate1, true_positive_rate1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG.fit(X_train, y_train)\n",
    "y_pred = LG.predict(X_test)\n",
    "y_prob = LG.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, y_prob[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve([int(i) for i in y_test],  y_prob[:,1])\n",
    "auc = roc_auc_score([int(i) for i in y_test], y_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, NN_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense layers only\n",
    "model=Sequential()\n",
    "model.add(Dense(12,input_dim=63,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax')) # output the results into two categories and l\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc']) #optimizer: help to minimize the loss because it is a classificati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, NN_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('NN model.h5')\n",
    "model = load_model('NN model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
